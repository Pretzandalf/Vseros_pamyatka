{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\nimport torch\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport os\n\n# Отключаем W&B\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Подготовка данных\ndata = pd.DataFrame({\n    'text': [\n        \"Текст 1: Сегодня погода солнечная и теплая. -- Текст 2: Планирую погулять в парке. Является продолжением? [да/нет]:\", \n        \"Текст 1: Вчера шел дождь, и мы остались дома. -- Текст 2: Завтра я планирую купить продукты. Является продолжением? [да/нет]:\",\n        \"Текст 1: Он купил билеты на самолет. -- Текст 2: Он с нетерпением ждал своей поездки. Является продолжением? [да/нет]:\", \n        \"Текст 1: Мы были на концерте вчера вечером. -- Текст 2: Сегодня я снова пойду на концерт. Является продолжением? [да/нет]:\",\n        \"Текст 1: Она сделала домашку за вечер. -- Текст 2: У нее не было времени на другие дела. Является продолжением? [да/нет]:\", \n        \"Текст 1: Я купил новую книгу по программированию. -- Текст 2: Я начал изучать новый язык программирования. Является продолжением? [да/нет]:\",\n        \"Текст 1: Он выиграл соревнования по шахматам. -- Текст 2: Он готовится к следующему турниру. Является продолжением? [да/нет]:\", \n        \"Текст 1: Мы отпраздновали Новый год с друзьями. -- Текст 2: В следующем году мы планируем провести праздник вместе. Является продолжением? [да/нет]:\", \n        \"Текст 1: Я занимаюсь спортом каждое утро. -- Текст 2: Сегодня я пропустил тренировку. Является продолжением? [да/нет]:\",\n        \"Текст 1: Вчера я потерял свой телефон. -- Текст 2: Сегодня мне удалось его найти. Является продолжением? [да/нет]:\",\n        \"Текст 1: Мы поехали на отдых на море. -- Текст 2: Мы планируем вернуться туда летом. Является продолжением? [да/нет]:\", \n        \"Текст 1: Я начал читать новую книгу по истории. -- Текст 2: Эта книга рассказывает о древних цивилизациях. Является продолжением? [да/нет]:\"\n    ],\n    'label': [1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0]  # 1 - продолжение, 0 - не продолжение\n})\n\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Загрузка токенизатора и модели\ntokenizer = GPT2Tokenizer.from_pretrained(\"sberbank-ai/rugpt3medium_based_on_gpt2\")\nmodel = GPT2ForSequenceClassification.from_pretrained(\"sberbank-ai/rugpt3medium_based_on_gpt2\", num_labels=2)\n\n# Токенизация\n# Токенизация\ndef tokenize_function(examples):\n    return tokenizer(list(examples[\"text\"]), padding=\"max_length\", truncation=True, max_length=128)\n\ntrain_encodings = tokenize_function(train_data)\nval_encodings = tokenize_function(val_data)\n\n\n# Подготовка датасетов\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = Dataset(train_encodings, train_data['label'].tolist())\nval_dataset = Dataset(val_encodings, val_data['label'].tolist())\n\n# Настройка тренировки\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    logging_steps=10,\n    logging_first_step=True, \n    learning_rate=2e-5,\n    per_device_train_batch_size=2,  \n    per_device_eval_batch_size=2,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    #report_to=\"none\",\n)\n\n# Создание Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Тренировка\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T20:51:50.811024Z","iopub.execute_input":"2024-11-13T20:51:50.811669Z","iopub.status.idle":"2024-11-13T20:52:16.912846Z","shell.execute_reply.started":"2024-11-13T20:51:50.811629Z","shell.execute_reply":"2024-11-13T20:52:16.911978Z"}},"outputs":[{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/rugpt3medium_based_on_gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15/15 00:22, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.043200</td>\n      <td>0.532534</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.658500</td>\n      <td>0.544886</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.658500</td>\n      <td>0.703137</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=15, training_loss=0.6941921869913737, metrics={'train_runtime': 22.8694, 'train_samples_per_second': 1.181, 'train_steps_per_second': 0.656, 'total_flos': 6268772155392.0, 'train_loss': 0.6941921869913737, 'epoch': 3.0})"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}