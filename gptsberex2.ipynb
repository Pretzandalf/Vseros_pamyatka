{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\nimport torch\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport os\n\n# Отключаем W&B\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Подготовка данных\ndata = pd.DataFrame({\n    'text': [\n        \"Текст 1: Сегодня погода солнечная и теплая. -- Текст 2: Планирую погулять в парке. Является продолжением? [да/нет]:\", \n        \"Текст 1: Вчера шел дождь, и мы остались дома. -- Текст 2: Завтра я планирую купить продукты. Является продолжением? [да/нет]:\",\n        \"Текст 1: Он купил билеты на самолет. -- Текст 2: Он с нетерпением ждал своей поездки. Является продолжением? [да/нет]:\", \n        \"Текст 1: Мы были на концерте вчера вечером. -- Текст 2: Сегодня я снова пойду на концерт. Является продолжением? [да/нет]:\",\n        \"Текст 1: Она сделала домашку за вечер. -- Текст 2: У нее не было времени на другие дела. Является продолжением? [да/нет]:\", \n        \"Текст 1: Я купил новую книгу по программированию. -- Текст 2: Я начал изучать новый язык программирования. Является продолжением? [да/нет]:\",\n        \"Текст 1: Он выиграл соревнования по шахматам. -- Текст 2: Он готовится к следующему турниру. Является продолжением? [да/нет]:\", \n        \"Текст 1: Мы отпраздновали Новый год с друзьями. -- Текст 2: В следующем году мы планируем провести праздник вместе. Является продолжением? [да/нет]:\", \n        \"Текст 1: Я занимаюсь спортом каждое утро. -- Текст 2: Сегодня я пропустил тренировку. Является продолжением? [да/нет]:\",\n        \"Текст 1: Вчера я потерял свой телефон. -- Текст 2: Сегодня мне удалось его найти. Является продолжением? [да/нет]:\",\n        \"Текст 1: Мы поехали на отдых на море. -- Текст 2: Мы планируем вернуться туда летом. Является продолжением? [да/нет]:\", \n        \"Текст 1: Я начал читать новую книгу по истории. -- Текст 2: Эта книга рассказывает о древних цивилизациях. Является продолжением? [да/нет]:\"\n    ],\n    'label': [1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0]  # 1 - продолжение, 0 - не продолжение\n})\n\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Загрузка токенизатора и модели\ntokenizer = GPT2Tokenizer.from_pretrained(\"sberbank-ai/rugpt3medium_based_on_gpt2\")\nmodel = GPT2ForSequenceClassification.from_pretrained(\"sberbank-ai/rugpt3medium_based_on_gpt2\", num_labels=2)\n\n# Токенизация\ndef tokenize_function(examples):\n    return tokenizer(list(examples[\"text\"]), padding=\"max_length\", truncation=True, max_length=128)\n\ntrain_encodings = tokenize_function(train_data)\nval_encodings = tokenize_function(val_data)\n\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, log_loss\nimport numpy as np\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = np.argmax(pred.predictions, axis=-1)\n\n    # Accuracy\n    accuracy = accuracy_score(labels, preds)\n\n    # Precision, Recall, F1\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, preds, average=\"binary\", zero_division=1\n    )\n\n    # ROC AUC (если метка 0/1)\n    if len(np.unique(labels)) == 2:\n        try:\n            roc_auc = roc_auc_score(labels, pred.predictions[:, 1])  # Использует вероятности положительного класса\n        except ValueError:\n            roc_auc = None\n    else:\n        roc_auc = None\n    \n    # Логарифмическая потеря\n    try:\n        logloss = log_loss(labels, pred.predictions)\n    except ValueError:\n        logloss = None\n\n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"roc_auc\": roc_auc,\n        \"log_loss\": logloss\n    }\n\n\n# Подготовка датасетов\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = Dataset(train_encodings, train_data['label'].tolist())\nval_dataset = Dataset(val_encodings, val_data['label'].tolist())\n\n# Настройка тренировки\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    logging_steps=10,\n    logging_first_step=True, \n    learning_rate=2e-5,\n    per_device_train_batch_size=2,  \n    per_device_eval_batch_size=2,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    #report_to=\"none\",\n)\n\n# Создание Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\n\n# Тренировка\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T07:25:13.856082Z","iopub.execute_input":"2024-11-14T07:25:13.856415Z","iopub.status.idle":"2024-11-14T07:26:05.113146Z","shell.execute_reply.started":"2024-11-14T07:25:13.856379Z","shell.execute_reply":"2024-11-14T07:26:05.112206Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff7a11b145e4bb5b989bbc524913a8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.61M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ab44efabab4af8bf5518f15b001edf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d988797963f4ecf8a6a1b3eab36ab7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/574 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c73afe4565cf42999de7baf099e09da7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/761 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fe19fa2e99c4ab7bfa315912784cb1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.73G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba84759221344626b68f17673704a70f"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/rugpt3medium_based_on_gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15/15 00:09, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Roc Auc</th>\n      <th>Log Loss</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n      <th>Steps Per Second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.717900</td>\n      <td>0.189350</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>None</td>\n      <td>None</td>\n      <td>0.123000</td>\n      <td>24.390000</td>\n      <td>16.260000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.822900</td>\n      <td>0.066057</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>None</td>\n      <td>None</td>\n      <td>0.064700</td>\n      <td>46.369000</td>\n      <td>30.913000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.822900</td>\n      <td>0.088495</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>None</td>\n      <td>None</td>\n      <td>0.069500</td>\n      <td>43.169000</td>\n      <td>28.779000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"None\" of type <class 'NoneType'> for key \"eval/roc_auc\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"None\" of type <class 'NoneType'> for key \"eval/log_loss\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"None\" of type <class 'NoneType'> for key \"eval/roc_auc\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"None\" of type <class 'NoneType'> for key \"eval/log_loss\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"None\" of type <class 'NoneType'> for key \"eval/roc_auc\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"None\" of type <class 'NoneType'> for key \"eval/log_loss\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=15, training_loss=0.7925993283589681, metrics={'train_runtime': 10.9484, 'train_samples_per_second': 2.466, 'train_steps_per_second': 1.37, 'total_flos': 6268772155392.0, 'train_loss': 0.7925993283589681, 'epoch': 3.0})"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}