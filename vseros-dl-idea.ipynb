{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **ЭТОТ НАУТБУК ЛУЧШЕ ВСЕГО ОПИСЫВАЕТ СЛОВО ПСЕВДОКОД БУДЕТ НЕВЕРОЯТНОЕ КОЛИЧЕСТВО БАГОВ, БЕЗ ТРЕЙНА НЕВОЗМОЖНО ЗАПРЕДИКТИТЬ**","metadata":{}},{"cell_type":"code","source":"!pip install -U accelerate\n!pip install transformers==4.45.2 sentence-transformers==3.1.1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport numpy as np\nimport random\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments, SentencesDataset, losses, CrossEncoder, InputExample, evaluation\nfrom sklearn.metrics.pairwise import cosine_similarity\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-11T20:43:35.915710Z","iopub.execute_input":"2024-11-11T20:43:35.916231Z","iopub.status.idle":"2024-11-11T20:43:35.922005Z","shell.execute_reply.started":"2024-11-11T20:43:35.916155Z","shell.execute_reply":"2024-11-11T20:43:35.920656Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"seed = 42\n\nrandom.seed(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\n\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T20:43:36.353758Z","iopub.execute_input":"2024-11-11T20:43:36.354852Z","iopub.status.idle":"2024-11-11T20:43:36.366615Z","shell.execute_reply.started":"2024-11-11T20:43:36.354792Z","shell.execute_reply":"2024-11-11T20:43:36.365250Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df = pd.read_csv(path)\ntest = pd.read_csv(path)\n\ndf = pd.DataFrame({\n    'text1': [\"текст1\", \"текст2\", \"текст3\"],\n    'text2': [\"текст4\", \"текст5\", \"текст6\"],\n    'text3': [\"текст7\", \"текст8\", \"текст9\"],\n    'text4': [\"текст10\", \"текст11\", \"текст12\"],\n    'text5': [\"текст13\", \"текст14\", \"текст15\"],\n    'label': [0, 1, 0]\n})\n\nsubmission_example = pd.read_csv(\"submission_example.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.Variables_Predicts_Train = pd.DataFrame()\npd.Variables_Predicts_Test = pd.DataFrame()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **CrossEncoder**","metadata":{}},{"cell_type":"code","source":"Train, Val = train_test_split(df, test_size=0.2, random_state=42)\n\ntrain = [\n    InputExample(\n        texts=[f\"{row['context']}: {row['anchor']}\", row['target']], \n        label=row['score']\n    ) \n    for _, row in Train.iterrows()\n]\n\nval = [\n    InputExample(\n        texts=[f\"{row['context']}: {row['anchor']}\", row['target']], \n        label=row['score']\n    ) \n    for _, row in Val.iterrows()\n]\n\ntrain_dataloader = DataLoader(train, shuffle=True, batch_size=64)\nevaluator = CECorrelationEvaluator.from_input_examples(val, name=\"val-eval\")\n\nmodel = CrossEncoder('deepvk/USER-bge-m3', num_labels=1, device='cuda')\n\nmodel.fit(\n    train_dataloader=train_dataloader,\n    evaluator=evaluator,  \n    epochs=1,\n    warmup_steps=100,\n    evaluation_steps=100,\n    callback=validation_callback,\n)\n\n\nTest = pd.DataFrame({\n    'context': [\"контекст1\", \"контекст2\", \"контекст3\"],\n    'anchor': [\"якорь1\", \"якорь2\", \"якорь3\"],\n    'target': [\"цель1\", \"цель2\", \"цель3\"]\n})\n\n\ntest_texts = [[f\"{row['context']}: {row['anchor']}\", row['target']] for _, row in Test.iterrows()]\n\npredict = model.predict(test_texts)\nsubmission_example['label'] = predict\nsubmission_example.to_csv(\"submission.csv\", index=False)\nVariables_Predicts_Test['Baseline_pred'] = predict\n\ntrain_texts = [\n    [f\"{row['context']}: {row['anchor']}\", row['target']] for _, row in Train.iterrows()\n]\npredictions = model.predict(train_texts)\n\nVariables_Predicts_Train['Baseline_pred'] = predict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Base cos sim**","metadata":{}},{"cell_type":"code","source":"model = SentenceTransformer(\"deepvk/USER-bge-m3\")\n\ndef take_sim(df):\n    history = df[col for col in df.columns if col != ['label' , 'query']]\n    average_embeddings = np.array([\n    np.mean(model.encode(row[text_columns].tolist()), axis=0) for _, row in history.iterrows()\n    ])\n    query = df['query']\n    query_embeddings = model.encode(query.tolist())\n\n    similarities = cosine_similarity(average_embeddings, query_embeddings)\n    \n    return similarities\n    \npredict = take_sim(test)\nVariables_Predicts_Test['Baseline_pred_1'] = predict\nsubmission_example['label'] = predict\nsubmission_example.to_csv(\"submission.csv\", index=False)\n\n\npredict = take_sim(df)\nVariables_Predicts_Train['Baseline_pred_1'] = predict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Trainig Encoder**","metadata":{}},{"cell_type":"code","source":"Train, Val = train_test_split(train, test_size=0.2, random_state=42)\n\ntrain_dataset = Dataset.from_dict({\n    \"sentence1\": [row['combined'] for _, row in train.iterrows()],\n    \"sentence2\": [row['target'] for _, row in train.iterrows()],\n    \"score\": [row['score'] for _, row in train.iterrows()],\n})\n\nval_dataset = Dataset.from_dict({\n    \"sentence1\": [row['combined'] for _, row in Val.iterrows()],\n    \"sentence2\": [row['target'] for _, row in Val.iterrows()],\n    \"score\": [row['score'] for _, row in Val.iterrows()],\n})\n\nevaluator = evaluation.EmbeddingSimilarityEvaluator(\n    val_dataset[\"sentence1\"],\n    val_dataset[\"sentence2\"],\n    val_dataset[\"score\"]\n)\n\ntraining_args = SentenceTransformerTrainingArguments(\n    output_dir=\"/home/pret/PycharmProjects/Vseros_classification/Models/mpnet-base\",\n    num_train_epochs=3,\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    logging_dir=\"/home/pret/PycharmProjects/Vseros_classification/Models/\",\n    logging_steps=100,\n    save_steps=1000,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=64,\n    warmup_steps=100,\n    weight_decay=0.01,\n    save_total_limit=1,\n    load_best_model_at_end=True\n)\n\n# Инициализация модели и loss\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\nloss = losses.CoSENTLoss(model)\n\n# Обучение модели с Trainer\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    evaluator=evaluator,\n #   compute_metrics=compute_metrics,\n    loss=loss,\n #   callbacks=[CustomCallback()],\n    args=training_args\n)\n\ntrainer.train()\n\ndef take_sim(df):\n    history = df[col for col in df.columns if col != ['label' , 'query']]\n    average_embeddings = np.array([\n    np.mean(model.encode(row[text_columns].tolist()), axis=0) for _, row in history.iterrows()\n    ])\n    query = df['query']\n    query_embeddings = model.encode(query.tolist())\n\n    similarities = cosine_similarity(average_embeddings, query_embeddings)\n    \n    return similarities\n    \npredict = take_sim(test)\nVariables_Predicts_Test['Fine_tuning_pred'] = predict\nsubmission_example['label'] = predict\nsubmission_example.to_csv(\"submission.csv\", index=False)\n\n\npredict = take_sim(df)\nVariables_Predicts_Train['Fine_tuning_pred'] = predict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transform_text_columns(df, model):\n    for col in df.columns:\n        if col != 'lable'\n        embeddings = model.encode(df[col].tolist())\n        df[col] = list(np.array(embeddings))\n    return df\n\ndf = transform_text_columns(df)\ntest = transform_text_columns(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Catboost on embeddings**","metadata":{}},{"cell_type":"code","source":"for_cat_data = pd.DataFrame()\n\nembedding_cols = [col for col in df.columns if not in ['query', 'target']]\ndef compute_average_embedding(row, embedding_cols):\n    embeddings = np.array([row[col] for col in embedding_cols])\n    average = embeddings.mean(axis=0)\n    return average\n\ndf['average_embedding'] = df.apply(lambda row: compute_average_embedding(row), axis=1)\ntest['average_embedding'] = test.apply(lambda row: compute_average_embedding(row), axis=1)\n\ndef expand_embeddings(df, embedding_cols):\n    for embedding_col in embedding_cols:\n        # Создаем новые колонки для каждого элемента эмбеддинга\n        embedding_size = len(df[embedding_col][0])  # размер эмбеддинга\n        expanded_cols = {f\"{embedding_col}_{i}\": df[embedding_col].apply(lambda x: x[i]) for i in range(embedding_size)}\n        \n        # Добавляем новые колонки в DataFrame\n        df = pd.concat([df, pd.DataFrame(expanded_cols)], axis=1)\n        \n        # Удаляем исходный столбец с эмбеддингом\n        df = df.drop(columns=[embedding_col])\n    \n    return df\n\n# Применяем функцию для раскладывания эмбеддингов\nfor_cat_data = expand_embeddings(df, embedding_cols=['average_embedding', 'query'])\nfor_cat_data['target'] = df['target']\n\ntest_cat_data = expand_embeddings(df, embedding_cols=['average_embedding', 'query'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size=.20, random_state=2)\n\ntrain_df.reset_index(drop=True, \n               inplace=True)\nval_df.reset_index(drop=True, \n                 inplace=True)\n\ny = train_df['target']\nX = train_df.drop(columns = 'target')\ny_val = val_df['target']\nX_val = val_df.drop(columns = 'target')\n\n\nmodel = CatBoostClassifier(\n    #scale_pos_weight=scale_pos_weight_value,\n    auto_class_weights='SqrtBalanced',\n    #auto_class_weights='Balanced',\n    loss_function='Logloss',\n    eval_metric='AUC',\n    random_seed=42,\n    logging_level='Silent',\n    iterations=1000,\n    task_type=\"GPU\",\n    devices='0',\n    use_best_model=True,\n    early_stopping_rounds=100\n)\n\n\nmodel.fit(\n    X, y,\n    cat_features=cats,\n    eval_set=[(X_val, y_val)],\n    logging_level='Verbose', #Verbose\n)\n    \nprint(\" AUC по test: \", model.get_best_score()['validation']['AUC'])\n\npredict = model.predict_proba(test_cat_data)[:,1:].flatten()\nsubmission_example['label'] = predict\nsubmission_example.to_csv(\"submission.csv\", index=False)\nVariables_Predicts_Test['Catboost_emb'] = predict\n\npredict = model.predict(for_cat_data)\nVariables_Predicts_Train['Catboost_emb'] = predict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Catboost on values**","metadata":{}},{"cell_type":"code","source":"for_cat_data = pd.DataFrame()\nembedding_cols = [col for col in df.columns if col not in ['query', 'target']]\nquery_column = 'query'\n\n# Функция для вычисления косинусного сходства\ndef calculate_cosine_similarity(row, query_col, embedding_col):\n    return cosine_similarity([row[query_col]], [row[embedding_col]])[0, 0]\n\n# Заменяем эмбеддинги на косинусное сходство с `query`\nfor col in embedding_cols:\n    for_cat_data[col] = df.apply(lambda row: calculate_cosine_similarity(row, query_column, col), axis=1)\n    test_cat_data[col] = df.apply(lambda row: calculate_cosine_similarity(row, query_column, col), axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size=.20, random_state=2)\n\ntrain_df.reset_index(drop=True, \n               inplace=True)\nval_df.reset_index(drop=True, \n                 inplace=True)\n\ny = train_df['target']\nX = train_df.drop(columns = 'target')\ny_val = val_df['target']\nX_val = val_df.drop(columns = 'target')\n\n\nmodel = CatBoostClassifier(\n    #scale_pos_weight=scale_pos_weight_value,\n    auto_class_weights='SqrtBalanced',\n    #auto_class_weights='Balanced',\n    loss_function='Logloss',\n    eval_metric='AUC',\n    random_seed=42,\n    logging_level='Silent',\n    iterations=1000,\n    task_type=\"GPU\",\n    devices='0',\n    use_best_model=True,\n    early_stopping_rounds=100\n)\n\nmodel.fit(\n    X, y,\n    cat_features=cats,\n    eval_set=[(X_val, y_val)],\n    logging_level='Verbose', #Verbose\n)\n    \nprint(\" AUC по test: \", model.get_best_score()['validation']['AUC'])\n\npredict = model.predict_proba(test_cat_data)[:,1:].flatten()\nsubmission_example['label'] = predict\nsubmission_example.to_csv(\"submission.csv\", index=False)\nVariables_Predicts_Test['Catboost_emb'] = predict\n\npredict = model.predict(for_cat_data)\nVariables_Predicts_Train['Catboost_emb'] = predict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Bert embmean**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom transformers import AutoModel, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn.functional import cosine_similarity\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Устройство для работы с моделью\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Класс для модели с двумя потоками для query и positive/negative\nclass EmbeddingModel(nn.Module):\n    def __init__(self, model_name=\"USER-bge-m3\"):\n        super().__init__()\n        self.device = device\n        self.embedding_model = AutoModel.from_pretrained(model_name).to(self.device)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    def forward(self, input_texts):\n        encoding = self.tokenizer.batch_encode_plus(\n            input_texts,\n            padding=\"max_length\",\n            max_length=128,\n            truncation=True,\n            return_tensors='pt',\n            add_special_tokens=True\n        )\n        input_ids = encoding['input_ids'].to(self.device)\n        attention_mask = encoding['attention_mask'].to(self.device)\n\n        # Извлекаем эмбеддинги текста\n        outputs = self.embedding_model(input_ids=input_ids, attention_mask=attention_mask)\n        embeddings = outputs.last_hidden_state.mean(dim=1)  # Усредняем по длине последовательности\n        return embeddings\n\n# Dataset для DataLoader\nclass ContrastiveTextDataset(Dataset):\n    def __init__(self, queries, positives, labels):\n        self.queries = queries\n        self.positives = positives\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return [self.queries[idx], self.positives[idx], self.labels[idx]]\n\n# Класс для обучения и валидации с косинусным расстоянием\nclass ContrastiveTrainer:\n    def __init__(self, model):\n        self.model = model\n\n    def train(self, dataloader, optimizer, loss_func, epochs, val_dataloader):\n        self.model.train()\n        for epoch in range(epochs):\n            for queries, positives, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", colour=\"GREEN\"):\n                labels = labels.to(device).float()\n                optimizer.zero_grad()\n\n                # Получение эмбеддингов для query и positive/negative\n                query_embeddings = self.model(queries)\n                positive_embeddings = self.model(positives)\n\n                # Расчет косинусной близости\n                cos_sim = cosine_similarity(query_embeddings, positive_embeddings)\n                loss = loss_func(cos_sim, labels)  # Контрастивная функция потерь\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n\n            self.evaluate(val_dataloader, loss_func)\n\n    def evaluate(self, dataloader, loss_func):\n        self.model.eval()\n        test_loss, correct = 0, 0\n        size = len(dataloader.dataset)\n        num_batches = len(dataloader)\n        \n        with torch.no_grad():\n            for queries, positives, labels in tqdm(dataloader, desc=\"Evaluation\", colour=\"CYAN\"):\n                labels = labels.to(device).float()\n                query_embeddings = self.model(queries)\n                positive_embeddings = self.model(positives)\n                cos_sim = cosine_similarity(query_embeddings, positive_embeddings)\n                loss = loss_func(cos_sim, labels)\n                test_loss += loss.item()\n                preds = (cos_sim > 0.5).float()  # Прогнозы на основе косинусной близости\n                correct += (preds == labels).type(torch.float).sum().item()\n\n        accuracy = correct / size\n        avg_loss = test_loss / num_batches\n        print(f\"Validation: \\nAccuracy: {accuracy*100:.1f}%, Avg loss: {avg_loss:.4f} \\n\")\n\n    def create_result(self, test_data):\n        self.model.eval()\n        queries = test_data['query'].values.tolist()\n        positives = test_data['positive'].values.tolist()\n        \n        results = []\n        for query, positive in tqdm(zip(queries, positives), desc=\"Generating Results\", colour=\"CYAN\"):\n            query_emb = self.model([query])\n            positive_emb = self.model([positive])\n            cos_sim = cosine_similarity(query_emb, positive_emb)\n            results.append(cos_sim.item())\n\n        submission = pd.DataFrame({'id': test_data['id'], 'similarity': results})\n        return submission\n\n    def predict_proba(self, queries, positives):\n        self.model.eval()\n        query_emb = self.model(queries)\n        positive_emb = self.model(positives)\n        cos_sim = cosine_similarity(query_emb, positive_emb)\n        return cos_sim\n\n# Сплит данных\nqueries = train_data['query'].values.tolist()\npositives = train_data['positive'].values.tolist()\nlabels = train_data['label'].values.tolist()\nX_train_q, X_val_q, X_train_p, X_val_p, y_train, y_val = train_test_split(\n    queries, positives, labels, test_size=0.2)\n\n# Создание датасетов и DataLoader\ntrain_dataset = ContrastiveTextDataset(X_train_q, X_train_p, y_train)\nval_dataset = ContrastiveTextDataset(X_val_q, X_val_p, y_val)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)\n\n# Инициализация модели и компонентов обучения\nmodel = EmbeddingModel()\nepochs = 10\nlearning_rate = 1e-6\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-4, steps_per_epoch=len(train_loader), epochs=epochs)\nloss_func = nn.CosineEmbeddingLoss()\n\n# Обучение и валидация\ntrainer = ContrastiveTrainer(model)\ntrainer.train(train_loader, optimizer, loss_func, epochs, val_loader)\n\n# Создание предсказаний для теста\npredict_test = trainer.predict_proba(test_data['query'].values.tolist(), test_data['positive'].values.tolist())\ntest_data['label'] = predict_test.flatten()\n\n# Сохранение предсказаний в CSV для подачи в соревнование\ntest_data.to_csv(\"submission.csv\", index=False)\n\n# Ансамблирование (например, добавление прогнозов других моделей)\nVariables_Predicts_Test['Model_emb'] = predict_test.flatten()\nVariables_Predicts_Train['Model_emb'] = trainer.predict_proba(train_data['query'].values.tolist(), train_data['positive'].values.tolist()).flatten()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Query Bert**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom transformers import AutoModel, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.nn.functional import cosine_similarity\n\n# Устройство для работы с моделью\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Класс модели с расчетом эмбеддингов текста и query\nclass EmbeddingSimilarityModel(nn.Module):\n    def __init__(self, model_name=\"USER-bge-m3\"):\n        super().__init__()\n        self.device = device\n        self.embedding_model = AutoModel.from_pretrained(model_name).to(self.device)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    def forward(self, text_inputs, query_inputs):\n        # Кодируем текстовые эмбеддинги\n        text_encoding = self.tokenizer.batch_encode_plus(\n            text_inputs,\n            padding=\"max_length\",\n            max_length=128,\n            truncation=True,\n            return_tensors='pt',\n            add_special_tokens=True\n        )\n        query_encoding = self.tokenizer.batch_encode_plus(\n            query_inputs,\n            padding=\"max_length\",\n            max_length=128,\n            truncation=True,\n            return_tensors='pt',\n            add_special_tokens=True\n        )\n\n        # Получаем эмбеддинги для текста и запроса\n        text_embeds = self.embedding_model(\n            input_ids=text_encoding['input_ids'].to(self.device),\n            attention_mask=text_encoding['attention_mask'].to(self.device)\n        ).last_hidden_state.mean(dim=1)\n\n        query_embeds = self.embedding_model(\n            input_ids=query_encoding['input_ids'].to(self.device),\n            attention_mask=query_encoding['attention_mask'].to(self.device)\n        ).last_hidden_state.mean(dim=1)\n\n        # Считаем косинусное расстояние\n        cos_similarity = cosine_similarity(text_embeds, query_embeds, dim=-1)\n        return cos_similarity\n\n# Dataset для DataLoader\nclass TextPairDataset(Dataset):\n    def __init__(self, texts, queries, labels):\n        self.texts = texts\n        self.queries = queries\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return [self.texts[idx], self.queries[idx], self.labels[idx]]\n\n# Класс для обучения и валидации\nclass ModelTrainer:\n    def __init__(self, model):\n        self.model = model\n\n    def train(self, dataloader, optimizer, loss_func, epochs, val_dataloader):\n        self.model.train()\n        for epoch in range(epochs):\n            for texts, queries, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", colour=\"GREEN\"):\n                labels = labels.to(device).float()  # Для CosineEmbeddingLoss метки -1 или 1\n                optimizer.zero_grad()\n                cos_sim = self.model(texts, queries)\n                loss = loss_func(cos_sim, labels)  # Потеря на косинусном расстоянии\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n            \n            self.evaluate(val_dataloader, loss_func)\n\n            # Сохранение модели на каждой эпохе\n            checkpoint = {'model': self.model, 'state_dict': self.model.state_dict()}\n            torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}.pth')\n\n    def evaluate(self, dataloader, loss_func):\n        self.model.eval()\n        test_loss, correct = 0, 0\n        size = len(dataloader.dataset)\n        num_batches = len(dataloader)\n        \n        with torch.no_grad():\n            for texts, queries, labels in tqdm(dataloader, desc=\"Evaluation\", colour=\"CYAN\"):\n                labels = labels.to(device).float()\n                cos_sim = self.model(texts, queries)\n                loss = loss_func(cos_sim, labels)\n                test_loss += loss.item()\n\n        avg_loss = test_loss / num_batches\n        print(f\"Validation: Avg loss: {avg_loss:.4f} \\n\")\n\n    def create_result(self, test_data, query_data):\n        text_inputs = test_data['text'].values.tolist()\n        query_inputs = query_data['query'].values.tolist()\n        similarities = []\n\n        with torch.no_grad():\n            for text, query in zip(text_inputs, query_inputs):\n                cos_sim = self.model([text], [query])\n                similarities.append(cos_sim.item())\n\n        submission = pd.DataFrame({'id': test_data['id'], 'similarity_score': similarities})\n        return submission\n\n    def predict_proba(self, queries, positives):\n        self.model.eval()\n        cos_sim = self.model(positives.values.tolist(), queries.values.tolist())\n        return cos_sim\n\n# Сплит данных\nX_train, X_val, y_train, y_val = train_test_split(train_data[['text', 'query']].values.tolist(), train_data.target.values.tolist(), test_size=0.2)\n\n# Создание датасетов и DataLoader\ntrain_dataset = TextPairDataset([x[0] for x in X_train], [x[1] for x in X_train], y_train)\nval_dataset = TextPairDataset([x[0] for x in X_val], [x[1] for x in X_val], y_val)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)\n\n# Инициализация модели и компонентов обучения\nmodel = EmbeddingSimilarityModel()\nepochs = 10\nlearning_rate = 1e-6\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-4, steps_per_epoch=len(train_loader), epochs=epochs)\nloss_func = nn.BCEWithLogitsLoss()\n\n# Обучение и валидация\ntrainer = ModelTrainer(model)\ntrainer.train(train_loader, optimizer, loss_func, epochs, val_loader)\n\npredict_test = trainer.predict_proba(test_data['positive'], test_data['query']).flatten()\ntest_data['label'] = predict_test\n\n# Сохранение предсказаний в CSV для подачи в соревнование\ntest_data.to_csv(\"submission.csv\", index=False)\n\n# Ансамблирование (например, добавление прогнозов других моделей)\nVariables_Predicts_Test['Model_query_emb'] = predict_test\nVariables_Predicts_Train['Model_query_emb'] = trainer.predict_proba(train_data['positive'], train_data['query']).flatten()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Bertcat**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom transformers import AutoModel, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.nn.functional import cosine_similarity\n\n# Устройство для работы с моделью\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Класс модели с расчетом эмбеддингов текста и query\nclass EmbeddingConcatModel(nn.Module):\n    def __init__(self, model_name=\"USER-bge-m3\"):\n        super().__init__()\n        self.device = device\n        self.embedding_model = AutoModel.from_pretrained(model_name).to(self.device)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        \n        # Линейная голова из трех слоев после конкатенации эмбеддингов\n        self.classification_head = nn.Sequential(\n            nn.Linear(2 * self.embedding_model.config.hidden_size, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        ).to(device)\n\n    def forward(self, text_inputs, query_inputs):\n        # Кодируем текстовые эмбеддинги\n        text_encoding = self.tokenizer.batch_encode_plus(\n            text_inputs, padding=\"max_length\", max_length=128, truncation=True, return_tensors='pt', add_special_tokens=True\n        )\n        query_encoding = self.tokenizer.batch_encode_plus(\n            query_inputs, padding=\"max_length\", max_length=128, truncation=True, return_tensors='pt', add_special_tokens=True\n        )\n\n        # Получаем эмбеддинги для текста и запроса\n        text_embeds = self.embedding_model(\n            input_ids=text_encoding['input_ids'].to(self.device),\n            attention_mask=text_encoding['attention_mask'].to(self.device)\n        ).last_hidden_state.mean(dim=1)\n\n        query_embeds = self.embedding_model(\n            input_ids=query_encoding['input_ids'].to(self.device),\n            attention_mask=query_encoding['attention_mask'].to(self.device)\n        ).last_hidden_state.mean(dim=1)\n\n        # Конкатенируем эмбеддинги\n        combined_embeddings = torch.cat((text_embeds, query_embeds), dim=1)\n        logits = self.classification_head(combined_embeddings)\n\n        return logits\n\n# Dataset для DataLoader\nclass TextPairDataset(Dataset):\n    def __init__(self, texts, queries, labels):\n        self.texts = texts\n        self.queries = queries\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return [self.texts[idx], self.queries[idx], self.labels[idx]]\n\n# Класс для обучения и валидации\nclass ModelTrainer:\n    def __init__(self, model):\n        self.model = model\n\n    def train(self, dataloader, optimizer, loss_func, epochs, val_dataloader):\n        self.model.train()\n        for epoch in range(epochs):\n            for texts, queries, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", colour=\"GREEN\"):\n                labels = labels.to(device).float()\n                optimizer.zero_grad()\n                logits = self.model(texts, queries).squeeze()\n                loss = loss_func(logits, labels)\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n            \n            self.evaluate(val_dataloader, loss_func)\n\n            # Сохранение модели на каждой эпохе\n            checkpoint = {'model': self.model, 'state_dict': self.model.state_dict()}\n            torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}.pth')\n\n    def evaluate(self, dataloader, loss_func):\n        self.model.eval()\n        test_loss = 0\n        num_batches = len(dataloader)\n        \n        with torch.no_grad():\n            for texts, queries, labels in tqdm(dataloader, desc=\"Evaluation\", colour=\"CYAN\"):\n                labels = labels.to(device).float()\n                logits = self.model(texts, queries).squeeze()\n                loss = loss_func(logits, labels)\n                test_loss += loss.item()\n\n        avg_loss = test_loss / num_batches\n        print(f\"Validation: Avg loss: {avg_loss:.4f} \\n\")\n\n    def predict_proba(self, dataloader):\n        self.model.eval()\n        all_preds = []\n        \n        with torch.no_grad():\n            for texts, queries in tqdm(dataloader, desc=\"Predicting\", colour=\"CYAN\"):\n                logits = torch.sigmoid(self.model(texts, queries)).squeeze().cpu().numpy()\n                all_preds.extend(logits)\n        \n        return all_preds\n\n# Сплит данных\nX_train, X_val, y_train, y_val = train_test_split(\n    train_data[['text', 'query']].values.tolist(), train_data['target'].values.tolist(), test_size=0.2\n)\n\n# Создание датасетов и DataLoader\ntrain_dataset = TextPairDataset([x[0] for x in X_train], [x[1] for x in X_train], y_train)\nval_dataset = TextPairDataset([x[0] for x in X_val], [x[1] for x in X_val], y_val)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)\n\n# Инициализация модели и компонентов обучения\nmodel = EmbeddingConcatModel()\nepochs = 10\nlearning_rate = 1e-6\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-4, steps_per_epoch=len(train_loader), epochs=epochs)\nloss_func = nn.BCEWithLogitsLoss()\n\n# Обучение и валидация\ntrainer = ModelTrainer(model)\ntrainer.train(train_loader, optimizer, loss_func, epochs, val_loader)\n\n# Предсказания на тестовой выборке\ntest_dataset = TextPairDataset(test_data['text'].values.tolist(), test_data['query'].values.tolist(), [0] * len(test_data))\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n\npredict_test = trainer.predict_proba(test_loader)\ntest_data['label'] = predict_test\n\n# Сохранение предсказаний в CSV для подачи в соревнование\ntest_data.to_csv(\"submission.csv\", index=False)\n\n# Ансамблирование\nVariables_Predicts_Test['Model_cat_emb'] = predict_test\nVariables_Predicts_Train['Model_cat_emb'] = trainer.predict_proba(train_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}