{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":9509060,"sourceType":"datasetVersion","datasetId":5787947},{"sourceId":122598,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":103173,"modelId":127404}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from catboost import CatBoostClassifier\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom catboost import CatBoostClassifier, Pool, metrics, cv\nfrom tqdm import tqdm\nimport numpy as np\nimport re\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport wandb\nfrom transformers import get_cosine_schedule_with_warmup\n\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-09-29T19:56:27.864610Z","iopub.execute_input":"2024-09-29T19:56:27.864902Z","iopub.status.idle":"2024-09-29T19:56:34.981736Z","shell.execute_reply.started":"2024-09-29T19:56:27.864869Z","shell.execute_reply":"2024-09-29T19:56:34.980894Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Скачиваем и обрабатываем данные\ntrain_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')  \ntrain_data = train_data[['text', 'target']] \n\n\ntrain_data.loc[train_data['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target'] = 1\ntrain_data.loc[train_data['text'] == 'Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife', 'target'] = 1\ntrain_data.loc[train_data['text'] == 'To fight bioterrorism sir.', 'target'] = 1\ntrain_data.loc[train_data['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target'] = 1\ntrain_data.loc[train_data['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target'] = 1\ntrain_data.loc[train_data['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target'] = 1\ntrain_data.loc[train_data['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target'] = 1\ntrain_data.loc[train_data['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target'] = 1\ntrain_data.loc[train_data['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target'] = 0\ntrain_data.loc[train_data['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target'] = 1\ntrain_data.loc[train_data['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target'] = 0\ntrain_data.loc[train_data['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target'] = 1\ntrain_data.loc[train_data['text'] == \"Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do anything that leads to it #islam!\", 'target'] = 1\ntrain_data.loc[train_data['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target'] = 1\ntrain_data.loc[train_data['text'] == \"Caution: breathing may be hazardous to your health.\", 'target'] = 0\ntrain_data.loc[train_data['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target'] = 1\ntrain_data.loc[train_data['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target'] = 1\ntrain_data.loc[train_data['text'] == \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target'] = 0\n\n\ntest_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T19:56:34.983366Z","iopub.execute_input":"2024-09-29T19:56:34.983736Z","iopub.status.idle":"2024-09-29T19:56:35.097363Z","shell.execute_reply.started":"2024-09-29T19:56:34.983700Z","shell.execute_reply":"2024-09-29T19:56:35.096351Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# **Bert + catboost**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\n\nclass Bert(nn.Module):\n    def __init__(self, trainable = False):\n        super().__init__()\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.bert = AutoModel.from_pretrained('bert-base-uncased').to(self.device)\n        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n        if trainable == False:\n            for m in self.bert.modules():\n                for name, params in m.named_parameters():\n                    params.requires_grad = False\n        self.target_indx = 0\n        \n\n    def forward(self, input):\n\n        encoding = self.tokenizer.batch_encode_plus(\n            input,  # List of input texts\n            padding=\"max_length\",\n            max_length=512,  # Pad to the maximum sequence length\n            truncation=True,  # Truncate to the maximum sequence length if necessary\n            return_tensors='pt',  # Return PyTorch tensors\n            add_special_tokens=True  # Add special tokens CLS and SEP\n        )\n\n        input_ids = encoding['input_ids'].to(self.device)\n        attention_mask = encoding['attention_mask'].to(self.device)\n\n        out = self.bert(input_ids, attention_mask, output_hidden_states=True)\n        out = out[0][:,self.target_indx,:]\n\n        return out\n    \nclass dataset_bilder(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n    \n    \nclass bert_catboost(nn.Module):\n    def __init__(self, params):\n        super().__init__()\n        self.bert = Bert()\n        self.catboost = CatBoostClassifier(**params)\n        \n    def train_catboost(self, train, val, batch_train_size, batch_val_size):\n        bert_output = []\n        bert_output_val = []\n        target = []\n        target_val = []\n        \n        for X_train, y_train in tqdm(train, desc=\"Train\", colour=\"CYAN\"):\n            if y_train.shape[0] == batch_train_size:\n                bert_output.append(self.bert(X_train).tolist())\n                target.append(y_train.tolist())\n            \n        for X_val, y_val in tqdm(val, desc=\"Val\", colour=\"CYAN\"):\n            if y_val.shape[0] == batch_val_size:\n                bert_output_val.append(self.bert(X_val).tolist())\n                target_val.append(y_val.tolist())\n        \n        bert_output = np.array(bert_output)\n        bert_output = bert_output.reshape(-1, bert_output.shape[-1])\n        bert_output_val = np.array(bert_output_val)\n        bert_output_val = bert_output_val.reshape(-1, bert_output_val.shape[-1])\n        \n        target = np.array(target)\n        target = target.reshape(-1)\n        target_val = np.array(target_val)\n        target_val = target_val.reshape(-1)\n        \n        self.catboost.fit(\n            bert_output, target,\n            eval_set=(bert_output_val, target_val),\n            #logging_level='Verbose',  # you can uncomment this for text output\n            plot=True\n        );\n        \n        \n    def forward(self, input):\n        output = self.bert(input).tolist()\n        output = self.catboost.predict(output)\n        \n        return output\n\n    \nclass model_testment(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        \n    def take_accuracy(self,test_data):\n        first_iter = True\n        for X, y in tqdm(test_data, desc=\"Testment\", colour=\"CYAN\"):\n            if first_iter == True:\n                model_prediction = self.model(X)\n                target = y.numpy() \n                first_iter = False\n            else:\n                model_prediction = np.concatenate((model_prediction, self.model(X)), axis = 0)\n                target = np.concatenate((target, y.numpy()), axis = 0)\n        \n        return len(model_prediction[model_prediction == target])/len(model_prediction)\n    \n    def create_result(self, pd_test):\n        texts = pd_test['text'].values.tolist()   \n        model_output = []\n        for text in texts:\n            model_output.append(self.model([text])[0])\n        model_output = np.array(model_output)\n        \n        submission = pd.DataFrame({'id': pd_test['id'], 'target': model_output})\n        \n        return submission\n    \n    def forward(self, input):\n        return self.model(input)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T09:53:22.673155Z","iopub.execute_input":"2024-09-28T09:53:22.673554Z","iopub.status.idle":"2024-09-28T09:53:22.700107Z","shell.execute_reply.started":"2024-09-28T09:53:22.673509Z","shell.execute_reply":"2024-09-28T09:53:22.699112Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split  # To split data into training and testing sets\n#Train-test split\n\nX_train, X_val, y_train, y_val = train_test_split(train_data.text.values.tolist(), train_data.target.values.tolist(), test_size=0.01)\n\ntrain_dataset = dataset_bilder(X_train, y_train)\nval_dataset = dataset_bilder(X_val, y_val)\n\nbatch_train_size = 64\nbatch_val_size = 64\n\ntrain = DataLoader(\n        train_dataset,\n        batch_size=batch_train_size,\n        num_workers=4,\n        shuffle=True,\n        collate_fn=None,\n    )\n\nval = DataLoader(\n        val_dataset,\n        batch_size=batch_val_size,\n        num_workers=4,\n        shuffle=False,\n        collate_fn=None,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-09-28T09:53:22.701970Z","iopub.execute_input":"2024-09-28T09:53:22.702662Z","iopub.status.idle":"2024-09-28T09:53:22.721949Z","shell.execute_reply.started":"2024-09-28T09:53:22.702626Z","shell.execute_reply":"2024-09-28T09:53:22.721008Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nparams = {\n    'iterations': 500,\n    'learning_rate': 0.01,\n    'eval_metric': metrics.Accuracy(),\n    'random_seed': 42,\n    'logging_level': 'Silent',\n    'use_best_model': True,\n    'task_type' : 'GPU'\n}\n\nmodel = bert_catboost(params)\nmodel.train_catboost(train, val, batch_train_size, batch_val_size)\n\nacc = model_testment(model).take_accuracy(val)\nprint('accuracy: ' + str(acc))\n\nsubmission = model_testment(model).create_result(test_data)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T09:53:22.723294Z","iopub.execute_input":"2024-09-28T09:53:22.723684Z","iopub.status.idle":"2024-09-28T09:57:30.041764Z","shell.execute_reply.started":"2024-09-28T09:53:22.723642Z","shell.execute_reply":"2024-09-28T09:57:30.040798Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fee8ce2449c465db4ebe1ed59f21bd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04e58d562060427eba1817ca8cd6459c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6cd0470abd7405a8baf49d33cfe9fe8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6638ab2e04394b8db0ac45518ce97969"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ffa7b2db2da49a3add8b2bca7d69573"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nTrain: 100%|\u001b[36m██████████\u001b[0m| 118/118 [02:01<00:00,  1.03s/it]\nVal: 100%|\u001b[36m██████████\u001b[0m| 2/2 [00:01<00:00,  1.72it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87d010975c584ed9981ac8193eb6a79c"}},"metadata":{}},{"name":"stderr","text":"Testment: 100%|\u001b[36m██████████\u001b[0m| 2/2 [00:01<00:00,  1.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"accuracy: 0.8051948051948052\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"acc = model_testment(model).take_accuracy(val)\nprint('accuracy: ' + str(acc))","metadata":{"execution":{"iopub.status.busy":"2024-09-27T09:00:00.577587Z","iopub.execute_input":"2024-09-27T09:00:00.580147Z","iopub.status.idle":"2024-09-27T09:00:27.517612Z","shell.execute_reply.started":"2024-09-27T09:00:00.580096Z","shell.execute_reply":"2024-09-27T09:00:27.516393Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Testment: 100%|\u001b[36m██████████\u001b[0m| 24/24 [00:26<00:00,  1.12s/it]","output_type":"stream"},{"name":"stdout","text":"accuracy: 0.8073089700996677\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"submission = model_testment(model).create_result(test_data)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T09:57:30.047178Z","iopub.execute_input":"2024-09-28T09:57:30.047504Z","iopub.status.idle":"2024-09-28T09:58:36.466776Z","shell.execute_reply.started":"2024-09-28T09:57:30.047465Z","shell.execute_reply":"2024-09-28T09:58:36.465956Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"#  **BertForSequenceClassification**","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    for parameter in model.parameters():\n        parameter.requires_grad = True\n    \n    return model\n\n\nclass Bert_classification(nn.Module): # создаем класс с бертом и токенайзером\n    def __init__(self, dowload_model_checkpoint = False):\n        super().__init__()\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        if dowload_model_checkpoint == False:\n            self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2 ).to(self.device)\n        else:\n            self.bert = load_checkpoint('/kaggle/input/bert_class/pytorch/default/1/checkpoint.pth')\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    def forward(self, input):\n        encoding = self.tokenizer.batch_encode_plus(\n            input,  # List of input texts\n            padding=\"max_length\",\n            max_length=128,  # Pad to the maximum sequence length\n            truncation=True,  # Truncate to the maximum sequence length if necessary\n            return_tensors='pt',  # Return PyTorch tensors\n            #add_special_tokens=True  # Add special tokens CLS and SEP\n        )\n\n        input_ids = encoding['input_ids'].to(self.device)\n        attention_mask = encoding['attention_mask'].to(self.device)\n\n        out = self.bert(input_ids, attention_mask)\n\n        return out.logits\n    \nclass dataset_bilder(Dataset): # для создания датасета\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n\n    \nclass model_usage(): # создем класс с кодом для обучения, тестирования и сохранению результата\n    def __init__(self, model):\n        self.model = model \n    \n    def train(self, dataloader, optimizer, loss_func, scheduler, epochs):\n        self.model.bert.train()\n        for _ in range(epochs):\n            for texts, labels in tqdm(dataloader, desc=\"Epoch\", colour=\"GREEN\"):\n                labels = labels.to(device)\n                optimizer.zero_grad()\n                output = self.model(texts)\n                labels = labels.long()\n                loss = loss_func(output, labels)\n                wandb.log({\"loss_val\": loss})\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n                \n            checkpoint = {'model': self.model,\n              'state_dict': self.model.state_dict()}\n\n            torch.save(checkpoint, 'checkpoint_'+ str(_) +'.pth')\n\n    def test(self, dataloader, loss_func):\n        self.model.eval()\n        testloss, correct = 0, 0\n        num_batches = len(dataloader)\n        size = len(dataloader.dataset)\n        \n        with torch.no_grad():\n            for texts, labels in tqdm(dataloader, desc=\"Eval\", colour=\"CYAN\"):\n                labels = labels.to(device)\n                output = self.model(texts)\n                labels = labels.long()\n                loss = loss_func(output, labels)\n                testloss += loss.item()\n                preds = torch.argmax(output, dim=1)\n                correct += (preds == labels).type(torch.float).sum().item()\n\n        correct /= size\n        testloss /= num_batches\n\n        print(f\"Testment: \\nAccuracy: {(100*correct):>0.1f}%, Avg loss: {testloss:>8f} \\n\")\n        \n    def create_result(self, pd_test):\n        texts = pd_test['text'].values.tolist()   \n        model_output = []\n        for text in tqdm(texts, desc=\"Creating\", colour=\"CYAN\"):\n            model_output.append(torch.argmax(self.model([text])).item())\n        model_output = np.array(model_output)\n        \n        submission = pd.DataFrame({'id': pd_test['id'], 'target': model_output})\n        \n        return submission","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:21:44.822846Z","iopub.execute_input":"2024-09-28T11:21:44.823814Z","iopub.status.idle":"2024-09-28T11:21:44.846492Z","shell.execute_reply.started":"2024-09-28T11:21:44.823770Z","shell.execute_reply":"2024-09-28T11:21:44.845445Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"## Сплитаем дату\nfrom sklearn.model_selection import train_test_split  # To split data into training and testing sets\n\nX_train, X_val, y_train, y_val = train_test_split(train_data.text.values.tolist(), train_data.target.values.tolist(), test_size=0.01)\n\n#Создаем даталоадер для обучения\ntrain_dataset = dataset_bilder(X_train, y_train)\nval_dataset = dataset_bilder(X_val, y_val)\n\nbatch_train_size = 128\nbatch_val_size = 128\n\ntrain = DataLoader(\n        train_dataset,\n        batch_size=batch_train_size,\n        num_workers=4,\n        shuffle=True,\n        collate_fn=None,\n    )\n\nval = DataLoader(\n        val_dataset,\n        batch_size=batch_val_size,\n        num_workers=4,\n        shuffle=False,\n        collate_fn=None,\n    )\n# Инициализируем модель\nmodel = Bert_classification(dowload_model_checkpoint = False)\n\n# Создаем вещи для обучения\nepochs = 10\nLR = 1e-6\noptimizer = optim.AdamW(model.bert.parameters(), lr = LR)\nloss_func = nn.CrossEntropyLoss()\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=120, num_training_steps= 59 * epochs)  # 226 - число батчей в датасете\n\n\nwandb.login()\nwandb.init(\n            project=\"BertForSequenceClassification\",\n            config={\n                \"config\": 'Tuning',\n                \"epoch\": str(epochs),\n                \"lr\": str(LR),\n            })\n\n# Обучаем\nmodel_f = model_usage(model)\nmodel_f.train(train, optimizer, loss_func, scheduler, epochs)                            \nBert_model = model_f.model\n\n# Тестируем\nmodel_f.test(val, loss_func)\n\n# Сохраняем результат на тест дате\nsubmission = model_f.create_result(test_data)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:21:46.429232Z","iopub.execute_input":"2024-09-28T11:21:46.429995Z","iopub.status.idle":"2024-09-28T11:21:50.674091Z","shell.execute_reply.started":"2024-09-28T11:21:46.429954Z","shell.execute_reply":"2024-09-28T11:21:50.672626Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f79ceaa6e0e8439db46031610ba2f9b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01e096ae06d246cbbeca02ec8f16aab3"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nKeyboardInterrupt\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"checkpoint = {'model': Bert_model.bert,\n              'state_dict': Bert_model.bert.state_dict()}\n\ntorch.save(checkpoint, 'checkpoint.pth')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **RobertaForSequenceClassification**","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForSequenceClassification\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nclass Roberta_classification(nn.Module): # создаем класс с бертом и токенайзером\n    def __init__(self):\n        super().__init__()\n        self.model_name = \"roberta-base\"\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.model = RobertaForSequenceClassification.from_pretrained(self.model_name, num_labels=2).to(self.device)\n        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name)\n\n    def forward(self, input):\n        encoding = self.tokenizer.batch_encode_plus(\n            input,  # List of input texts\n            padding=\"max_length\",\n            max_length=128,  # Pad to the maximum sequence length\n            truncation=True,  # Truncate to the maximum sequence length if necessary\n            return_tensors='pt',  # Return PyTorch tensors\n            add_special_tokens=True  # Add special tokens CLS and SEP\n        )\n\n        input_ids = encoding['input_ids'].to(self.device)\n        attention_mask = encoding['attention_mask'].to(self.device)\n\n        out = self.model(input_ids, attention_mask)\n\n        return out.logits\n    \nclass dataset_bilder(Dataset): # для создания датасета\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n\n    \nclass model_usage(): # создем класс с кодом для обучения, тестирования и сохранению результата\n    def __init__(self, model):\n        self.model = model \n    \n    def train(self, dataloader, optimizer, loss_func, epochs, val_data):\n        self.model.model.train()\n        for _ in range(epochs):\n            for texts, labels in tqdm(dataloader, desc=\"Epoch\", colour=\"GREEN\"):\n                labels = labels.to(device)\n                optimizer.zero_grad()\n                output = self.model(texts)\n                labels = labels.long()\n                loss = loss_func(output, labels)\n                wandb.log({\"loss_val\": loss})\n                loss.backward()\n                optimizer.step()\n            \n            self.test(val_data, loss_func)\n            \n            checkpoint = {'model': self.model,\n              'state_dict': self.model.state_dict()}\n\n            torch.save(checkpoint, 'checkpoint_'+ str(_) +'.pth')\n\n    def test(self, dataloader, loss_func):\n        self.model.eval()\n        testloss, correct = 0, 0\n        num_batches = len(dataloader)\n        size = len(dataloader.dataset)\n        \n        with torch.no_grad():\n            for texts, labels in tqdm(dataloader, desc=\"Eval\", colour=\"CYAN\"):\n                labels = labels.to(device)\n                output = self.model(texts)\n                labels = labels.long()\n                loss = loss_func(output, labels)\n                testloss += loss.item()\n                preds = torch.argmax(output, dim=1)\n                correct += (preds == labels).type(torch.float).sum().item()\n\n        correct /= size\n        testloss /= num_batches\n\n        print(f\"Testment: \\nAccuracy: {(100*correct):>0.1f}%, Avg loss: {testloss:>8f} \\n\")\n        \n    def create_result(self, pd_test):\n        texts = pd_test['text'].values.tolist()   \n        model_output = []\n        for text in tqdm(texts, desc=\"Creating\", colour=\"CYAN\"):\n            model_output.append(torch.argmax(self.model([text])).item())\n        model_output = np.array(model_output)\n        \n        submission = pd.DataFrame({'id': pd_test['id'], 'target': model_output})\n        \n        return submission","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:51:14.992374Z","iopub.execute_input":"2024-09-29T12:51:14.992671Z","iopub.status.idle":"2024-09-29T12:51:20.863532Z","shell.execute_reply.started":"2024-09-29T12:51:14.992639Z","shell.execute_reply":"2024-09-29T12:51:20.862199Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaTokenizer, RobertaForSequenceClassification\n\u001b[0;32m----> 3\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRoberta_classification\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule): \u001b[38;5;66;03m# создаем класс с бертом и токенайзером\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# Сплитаем дату\nfrom sklearn.model_selection import train_test_split  # To split data into training and testing sets\n\nX_train, X_val, y_train, y_val = train_test_split(train_data.text.values.tolist(), train_data.target.values.tolist(), test_size=0.2)\n\n#Создаем даталоадер для обучения\ntrain_dataset = dataset_bilder(X_train, y_train)\nval_dataset = dataset_bilder(X_val, y_val)\n\nbatch_train_size = 128\nbatch_val_size = 128\n\ntrain = DataLoader(\n        train_dataset,\n        batch_size=batch_train_size,\n        num_workers=4,\n        shuffle=True,\n        collate_fn=None,\n    )\n\nval = DataLoader(\n        val_dataset,\n        batch_size=batch_val_size,\n        num_workers=4,\n        shuffle=False,\n        collate_fn=None,\n    )\n# Инициализируем модель\nmodel = Roberta_classification()\n\n# Создаем вещи для обучения\nepochs = 10\nLR = 1e-6\noptimizer = optim.AdamW(model.model.parameters(), lr = LR)\nloss_func = nn.CrossEntropyLoss()\n\nwandb.login()\nwandb.init(\n            project=\"RobertaForSequenceClassification\",\n            config={\n                \"config\": 'Tuning',\n                \"epoch\": str(epochs),\n                \"lr\": str(LR),\n            })\n\n# Обучаем и тестируем на каждой эпохе\nmodel_f = model_usage(model)\nmodel_f.train(train, optimizer, loss_func, epochs, val)                            \nLamma_model = model_f.model\n\n\n# Сохраняем результат на тест дате\nsubmission = model_f.create_result(test_data)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:51:20.864281Z","iopub.status.idle":"2024-09-29T12:51:20.864626Z","shell.execute_reply.started":"2024-09-29T12:51:20.864455Z","shell.execute_reply":"2024-09-29T12:51:20.864472Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Roberta + CatBoost**","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaModel, RobertaTokenizer\n\nclass Bert(nn.Module):\n    def __init__(self, trainable = False):\n        super().__init__()\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.bert = RobertaModel.from_pretrained(\"roberta-base\").to(self.device)\n        self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n        if trainable == False:\n            for m in self.bert.modules():\n                for name, params in m.named_parameters():\n                    params.requires_grad = False\n        self.target_indx = 0\n        \n\n    def forward(self, input):\n\n        encoding = self.tokenizer.batch_encode_plus(\n            input,  # List of input texts\n            padding=\"max_length\",\n            max_length=512,  # Pad to the maximum sequence length\n            truncation=True,  # Truncate to the maximum sequence length if necessary\n            return_tensors='pt',  # Return PyTorch tensors\n            add_special_tokens=True  # Add special tokens CLS and SEP\n        )\n\n        input_ids = encoding['input_ids'].to(self.device)\n        attention_mask = encoding['attention_mask'].to(self.device)\n\n        out = self.bert(input_ids, attention_mask, output_hidden_states=True)\n        out = out[0][:,self.target_indx,:]\n\n        return out\n    \nclass dataset_bilder(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n    \n    \nclass bert_catboost(nn.Module):\n    def __init__(self, params):\n        super().__init__()\n        self.bert = Bert()\n        self.catboost = CatBoostClassifier(**params)\n        \n    def train_catboost(self, train, val, batch_train_size, batch_val_size):\n        bert_output = []\n        bert_output_val = []\n        target = []\n        target_val = []\n        \n        for X_train, y_train in tqdm(train, desc=\"Train\", colour=\"CYAN\"):\n            if y_train.shape[0] == batch_train_size:\n                bert_output.append(self.bert(X_train).tolist())\n                target.append(y_train.tolist())\n            \n        for X_val, y_val in tqdm(val, desc=\"Val\", colour=\"CYAN\"):\n            if y_val.shape[0] == batch_val_size:\n                bert_output_val.append(self.bert(X_val).tolist())\n                target_val.append(y_val.tolist())\n        \n        bert_output = np.array(bert_output)\n        bert_output = bert_output.reshape(-1, bert_output.shape[-1])\n        bert_output_val = np.array(bert_output_val)\n        bert_output_val = bert_output_val.reshape(-1, bert_output_val.shape[-1])\n        \n        target = np.array(target)\n        target = target.reshape(-1)\n        target_val = np.array(target_val)\n        target_val = target_val.reshape(-1)\n        \n        self.catboost.fit(\n            bert_output, target,\n            eval_set=(bert_output_val, target_val),\n            #logging_level='Verbose',  # you can uncomment this for text output\n            plot=True\n        );\n        \n        \n    def forward(self, input):\n        output = self.bert(input).tolist()\n        output = self.catboost.predict(output)\n        \n        return output\n\n    \nclass model_testment(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        \n    def take_accuracy(self,test_data):\n        first_iter = True\n        for X, y in tqdm(test_data, desc=\"Testment\", colour=\"CYAN\"):\n            if first_iter == True:\n                model_prediction = self.model(X)\n                target = y.numpy() \n                first_iter = False\n            else:\n                model_prediction = np.concatenate((model_prediction, self.model(X)), axis = 0)\n                target = np.concatenate((target, y.numpy()), axis = 0)\n        \n        return len(model_prediction[model_prediction == target])/len(model_prediction)\n    \n    def create_result(self, pd_test):\n        texts = pd_test['text'].values.tolist()   \n        model_output = []\n        for text in texts:\n            model_output.append(self.model([text])[0])\n        model_output = np.array(model_output)\n        \n        submission = pd.DataFrame({'id': pd_test['id'], 'target': model_output})\n        \n        return submission\n    \n    def forward(self, input):\n        return self.model(input)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T19:56:35.098733Z","iopub.execute_input":"2024-09-29T19:56:35.099029Z","iopub.status.idle":"2024-09-29T19:56:36.813673Z","shell.execute_reply.started":"2024-09-29T19:56:35.098998Z","shell.execute_reply":"2024-09-29T19:56:36.812710Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split  # To split data into training and testing sets\n#Train-test split\n\nX_train, X_val, y_train, y_val = train_test_split(train_data.text.values.tolist(), train_data.target.values.tolist(), test_size=0.01)\n\ntrain_dataset = dataset_bilder(X_train, y_train)\nval_dataset = dataset_bilder(X_val, y_val)\n\nbatch_train_size = 64\nbatch_val_size = 64\n\ntrain = DataLoader(\n        train_dataset,\n        batch_size=batch_train_size,\n        num_workers=4,\n        shuffle=True,\n        collate_fn=None,\n    )\n\nval = DataLoader(\n        val_dataset,\n        batch_size=batch_val_size,\n        num_workers=4,\n        shuffle=False,\n        collate_fn=None,\n    )\n\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nparams = {\n    'iterations': 500,\n    'learning_rate': 0.01,\n    'eval_metric': metrics.Accuracy(),\n    'random_seed': 42,\n    'logging_level': 'Silent',\n    'use_best_model': True,\n    'task_type' : 'GPU'\n}\n\nmodel = bert_catboost(params)\nmodel.train_catboost(train, val, batch_train_size, batch_val_size)\n\nacc = model_testment(model).take_accuracy(val)\nprint('accuracy: ' + str(acc))\n\nsubmission = model_testment(model).create_result(test_data)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T19:56:36.815752Z","iopub.execute_input":"2024-09-29T19:56:36.816240Z","iopub.status.idle":"2024-09-29T20:01:03.964886Z","shell.execute_reply.started":"2024-09-29T19:56:36.816203Z","shell.execute_reply":"2024-09-29T20:01:03.963919Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e5f384c29c042d6bf7af390df76db6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba1efb3dcfd14b0088bf16e9ccdf2bd1"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b074505a5b7e40909f63b2fd5d35adbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71a8bbf781344922b5911b1b77986d69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31d54051e54942c886f1353cf9a95b64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaef2f4474734a178f3e838bfe82bba9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nTrain: 100%|\u001b[36m██████████\u001b[0m| 118/118 [02:15<00:00,  1.15s/it]\nVal: 100%|\u001b[36m██████████\u001b[0m| 2/2 [00:01<00:00,  1.56it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83c6f48b36ae445eb44b8c429cd11ee3"}},"metadata":{}},{"name":"stderr","text":"Testment: 100%|\u001b[36m██████████\u001b[0m| 2/2 [00:01<00:00,  1.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"accuracy: 0.8571428571428571\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"submission = model_testment(model).create_result(test_data)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:01:03.966225Z","iopub.execute_input":"2024-09-29T20:01:03.966574Z","iopub.status.idle":"2024-09-29T20:02:17.087690Z","shell.execute_reply.started":"2024-09-29T20:01:03.966535Z","shell.execute_reply":"2024-09-29T20:02:17.086839Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# **ElectraForSequenceClassification**","metadata":{}},{"cell_type":"code","source":"from transformers import ElectraTokenizer, ElectraForSequenceClassification\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nclass Electra_classification(nn.Module): # создаем класс с бертом и токенайзером\n    def __init__(self):\n        super().__init__()\n        self.model_name = \"google/electra-small-discriminator\"\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.model = ElectraForSequenceClassification.from_pretrained(self.model_name, num_labels=2).to(self.device)\n        self.tokenizer = ElectraTokenizer.from_pretrained(self.model_name)\n\n    def forward(self, input):\n        encoding = self.tokenizer.batch_encode_plus(\n            input,  # List of input texts\n            padding=\"max_length\",\n            max_length=128,  # Pad to the maximum sequence length\n            truncation=True,  # Truncate to the maximum sequence length if necessary\n            return_tensors='pt',  # Return PyTorch tensors\n            add_special_tokens=True  # Add special tokens CLS and SEP\n        )\n\n        input_ids = encoding['input_ids'].to(self.device)\n        attention_mask = encoding['attention_mask'].to(self.device)\n\n        out = self.model(input_ids, attention_mask)\n\n        return out.logits\n    \nclass dataset_bilder(Dataset): # для создания датасета\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n\n    \nclass model_usage(): # создем класс с кодом для обучения, тестирования и сохранению результата\n    def __init__(self, model):\n        self.model = model \n    \n    def train(self, dataloader, optimizer, loss_func, epochs, val_data):\n        self.model.model.train()\n        for _ in range(epochs):\n            for texts, labels in tqdm(dataloader, desc=\"Epoch\", colour=\"GREEN\"):\n                labels = labels.to(device)\n                optimizer.zero_grad()\n                output = self.model(texts)\n                labels = labels.long()\n                loss = loss_func(output, labels)\n                wandb.log({\"loss_val\": loss})\n                loss.backward()\n                optimizer.step()\n            \n            self.test(val_data, loss_func)\n            \n            checkpoint = {'model': self.model,\n              'state_dict': self.model.state_dict()}\n\n            torch.save(checkpoint, 'checkpoint_'+ str(_) +'.pth')\n\n    def test(self, dataloader, loss_func):\n        self.model.eval()\n        testloss, correct = 0, 0\n        num_batches = len(dataloader)\n        size = len(dataloader.dataset)\n        \n        with torch.no_grad():\n            for texts, labels in tqdm(dataloader, desc=\"Eval\", colour=\"CYAN\"):\n                labels = labels.to(device)\n                output = self.model(texts)\n                labels = labels.long()\n                loss = loss_func(output, labels)\n                testloss += loss.item()\n                preds = torch.argmax(output, dim=1)\n                correct += (preds == labels).type(torch.float).sum().item()\n\n        correct /= size\n        testloss /= num_batches\n\n        print(f\"Testment: \\nAccuracy: {(100*correct):>0.1f}%, Avg loss: {testloss:>8f} \\n\")\n        \n    def create_result(self, pd_test):\n        texts = pd_test['text'].values.tolist()   \n        model_output = []\n        for text in tqdm(texts, desc=\"Creating\", colour=\"CYAN\"):\n            model_output.append(torch.argmax(self.model([text])).item())\n        model_output = np.array(model_output)\n        \n        submission = pd.DataFrame({'id': pd_test['id'], 'target': model_output})\n        \n        return submission","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:08:54.724060Z","iopub.execute_input":"2024-09-29T13:08:54.724863Z","iopub.status.idle":"2024-09-29T13:08:54.745230Z","shell.execute_reply.started":"2024-09-29T13:08:54.724820Z","shell.execute_reply":"2024-09-29T13:08:54.744316Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Сплитаем дату\nfrom sklearn.model_selection import train_test_split  # To split data into training and testing sets\n\nX_train, X_val, y_train, y_val = train_test_split(train_data.text.values.tolist(), train_data.target.values.tolist(), test_size=0.2)\n\n#Создаем даталоадер для обучения\ntrain_dataset = dataset_bilder(X_train, y_train)\nval_dataset = dataset_bilder(X_val, y_val)\n\nbatch_train_size = 128\nbatch_val_size = 128\n\ntrain = DataLoader(\n        train_dataset,\n        batch_size=batch_train_size,\n        num_workers=4,\n        shuffle=True,\n        collate_fn=None,\n    )\n\nval = DataLoader(\n        val_dataset,\n        batch_size=batch_val_size,\n        num_workers=4,\n        shuffle=False,\n        collate_fn=None,\n    )\n# Инициализируем модель\nmodel = Electra_classification()\n\n# Создаем вещи для обучения\nepochs = 8\nLR = 1e-5\noptimizer = optim.AdamW(model.model.parameters(), lr = LR)\nloss_func = nn.CrossEntropyLoss()\n\nwandb.login()\nwandb.init(\n            project=\"ElectraForSequenceClassification\",\n            config={\n                \"config\": 'Tuning',\n                \"epoch\": str(epochs),\n                \"lr\": str(LR),\n            })\n\n# Обучаем и тестируем на каждой эпохе\nmodel_f = model_usage(model)\nmodel_f.train(train, optimizer, loss_func, epochs, val)                            \nLamma_model = model_f.model\n\n\n# Сохраняем результат на тест дате\nsubmission = model_f.create_result(test_data)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:19:31.662890Z","iopub.execute_input":"2024-09-29T13:19:31.663580Z","iopub.status.idle":"2024-09-29T13:23:05.529249Z","shell.execute_reply.started":"2024-09-29T13:19:31.663541Z","shell.execute_reply":"2024-09-29T13:23:05.528210Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:vsxzzusc) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.018 MB of 0.018 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss_val</td><td>█████▇▆▇▅▆▅▅▄▅▄▄▅▃▄▃▃▄▃▃▄▃▃▃▃▂▃▂▃▂▃▂▁▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss_val</td><td>0.19628</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dutiful-vortex-6</strong> at: <a href='https://wandb.ai/pretzandalf_projects/ElectraForSequenceClassification/runs/vsxzzusc' target=\"_blank\">https://wandb.ai/pretzandalf_projects/ElectraForSequenceClassification/runs/vsxzzusc</a><br/> View project at: <a href='https://wandb.ai/pretzandalf_projects/ElectraForSequenceClassification' target=\"_blank\">https://wandb.ai/pretzandalf_projects/ElectraForSequenceClassification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240929_131426-vsxzzusc/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:vsxzzusc). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240929_131932-5q47v889</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/pretzandalf_projects/ElectraForSequenceClassification/runs/5q47v889' target=\"_blank\">floral-dawn-7</a></strong> to <a href='https://wandb.ai/pretzandalf_projects/ElectraForSequenceClassification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/pretzandalf_projects/ElectraForSequenceClassification' target=\"_blank\">https://wandb.ai/pretzandalf_projects/ElectraForSequenceClassification</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/pretzandalf_projects/ElectraForSequenceClassification/runs/5q47v889' target=\"_blank\">https://wandb.ai/pretzandalf_projects/ElectraForSequenceClassification/runs/5q47v889</a>"},"metadata":{}},{"name":"stderr","text":"Epoch: 100%|\u001b[32m██████████\u001b[0m| 60/60 [00:20<00:00,  2.91it/s]\nEval: 100%|\u001b[36m██████████\u001b[0m| 1/1 [00:00<00:00,  4.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Testment: \nAccuracy: 87.5%, Avg loss: 0.609466 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|\u001b[32m██████████\u001b[0m| 60/60 [00:20<00:00,  2.97it/s]\nEval: 100%|\u001b[36m██████████\u001b[0m| 1/1 [00:00<00:00,  7.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Testment: \nAccuracy: 87.5%, Avg loss: 0.481980 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|\u001b[32m██████████\u001b[0m| 60/60 [00:20<00:00,  2.98it/s]\nEval: 100%|\u001b[36m██████████\u001b[0m| 1/1 [00:00<00:00,  6.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Testment: \nAccuracy: 62.5%, Avg loss: 0.568511 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|\u001b[32m██████████\u001b[0m| 60/60 [00:20<00:00,  2.97it/s]\nEval: 100%|\u001b[36m██████████\u001b[0m| 1/1 [00:00<00:00,  6.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Testment: \nAccuracy: 87.5%, Avg loss: 0.403331 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|\u001b[32m██████████\u001b[0m| 60/60 [00:20<00:00,  2.96it/s]\nEval: 100%|\u001b[36m██████████\u001b[0m| 1/1 [00:00<00:00,  7.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Testment: \nAccuracy: 87.5%, Avg loss: 0.419730 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|\u001b[32m██████████\u001b[0m| 60/60 [00:20<00:00,  2.98it/s]\nEval: 100%|\u001b[36m██████████\u001b[0m| 1/1 [00:00<00:00,  6.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Testment: \nAccuracy: 87.5%, Avg loss: 0.441447 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|\u001b[32m██████████\u001b[0m| 60/60 [00:20<00:00,  2.97it/s]\nEval: 100%|\u001b[36m██████████\u001b[0m| 1/1 [00:00<00:00,  6.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Testment: \nAccuracy: 87.5%, Avg loss: 0.393552 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|\u001b[32m██████████\u001b[0m| 60/60 [00:20<00:00,  2.98it/s]\nEval: 100%|\u001b[36m██████████\u001b[0m| 1/1 [00:00<00:00,  6.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Testment: \nAccuracy: 87.5%, Avg loss: 0.395433 \n\n","output_type":"stream"},{"name":"stderr","text":"Creating: 100%|\u001b[36m██████████\u001b[0m| 3263/3263 [00:41<00:00, 78.81it/s]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"true_targets = pd.read_csv('/kaggle/input/leaked-data-test/submission_leaked_data.csv')\n\ntrue_targets['target'].values","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:34:29.664689Z","iopub.execute_input":"2024-09-29T20:34:29.665376Z","iopub.status.idle":"2024-09-29T20:34:29.687306Z","shell.execute_reply.started":"2024-09-29T20:34:29.665338Z","shell.execute_reply":"2024-09-29T20:34:29.686483Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([1, 1, 1, ..., 1, 1, 1])"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"submission['target'].values","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:32:48.527667Z","iopub.execute_input":"2024-09-29T13:32:48.528407Z","iopub.status.idle":"2024-09-29T13:32:48.542496Z","shell.execute_reply.started":"2024-09-29T13:32:48.528373Z","shell.execute_reply":"2024-09-29T13:32:48.541682Z"},"trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"         id  target\n0         0       1\n1         2       1\n2         3       1\n3         9       1\n4        11       1\n...     ...     ...\n3258  10861       1\n3259  10865       1\n3260  10868       1\n3261  10874       1\n3262  10875       1\n\n[3263 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3258</th>\n      <td>10861</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3259</th>\n      <td>10865</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3260</th>\n      <td>10868</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3261</th>\n      <td>10874</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3262</th>\n      <td>10875</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3263 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"comparison = true_targets.compare(submission)\n\n# Печать различий\nprint(comparison)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:34:31.962885Z","iopub.execute_input":"2024-09-29T20:34:31.963649Z","iopub.status.idle":"2024-09-29T20:34:31.986898Z","shell.execute_reply.started":"2024-09-29T20:34:31.963606Z","shell.execute_reply":"2024-09-29T20:34:31.985878Z"},"trusted":true},"outputs":[{"name":"stdout","text":"     target      \n       self other\n0       1.0   0.0\n2       1.0   0.0\n5       1.0   0.0\n17      0.0   1.0\n27      0.0   1.0\n...     ...   ...\n3234    1.0   0.0\n3241    0.0   1.0\n3242    1.0   0.0\n3246    1.0   0.0\n3262    1.0   0.0\n\n[823 rows x 2 columns]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}